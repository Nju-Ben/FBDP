# 151099044 胡犇 实验九

------
1. 下载并安装运行Spark

2. 在Spark上实现Project1的需求1（词频统计），可以用Scala也可以用Java或者Python。

### 一、环境配置
#### 1.下载spark并安装
不再赘述

#### 2.修改/etc/profile文件配置Spar环境变量
如图所示
![](https://i.imgur.com/5DgxvSt.png)

#### 3.测试Pyspark
启动pyspark命令行shell
```linux
bin/pyspark
```
运行截图

![](https://i.imgur.com/D5cVB6M.png)

简单运行脚本后,localhost:4040截图

![](https://i.imgur.com/MljnFuH.png)

### 二、编写spark程序实现词频统计
#### 1.输入数据说明:
输入数据为project2中使用了我自己处理的合并了标题的txt，样式如下:

![](https://i.imgur.com/dJfEFKG.png)


#### 2.python代码
```python
	# _*_ coding: utf-8 _*_
	from pyspark import SparkConf, SparkContext  
	import jieba
	import sys
	reload(sys)
	sys.setdefaultencoding( "utf-8" )
	from operator import add
	
	if __name__ == "__main__":

		#利用jieba进行分词并获取长度大于2的词语
	
		conf = SparkConf().setAppName("wordSplit")  
		conf.setMaster("local")  
		sc= SparkContext(conf = conf)  
		with open ("/home/huben/testdata.txt") as file:
		    data=file.read()
		cut = jieba.lcut(data)
		list=[]
		for i in cut:
		    if len(i)>=2:
			list.append(i)

		#将分词结果转化为rdd格式，并对词语和词频进行map和reduce操作

		lines = sc.parallelize(list)
		#counts = lines.flatMap(lambda x: x.split(' ')) \
		counts = lines.map(lambda x: (x, 1)) \
			  .reduceByKey(add).sortByKey()

		#收集结果转换为字典形式并排序打印结果

		output = counts.collect()
		result={}
		for (word, count) in output:
		    result[str(word)]=count
		y=sorted(result.items(), key=lambda result:result[1], reverse = True)
		for i in y:
		    if i[1]>int(int(sys.argv[1])):
			print("%s: %i" % (i[0], i[1]))
```

#### 3.运行示意
终端运行命令，k为要求输出的最小的词频,这儿k为1000，这儿直接利用spark-submit命令可以直接运行python脚本，比在shell里输入方便，也比hadoop需要打jar包方便

![](https://i.imgur.com/PaT2rJk.png)

显示结果

![](https://i.imgur.com/R2E8Sgz.png)

![](https://i.imgur.com/3LdSuFs.png)

正在尝试用spark补充project2,望老师暂缓我的评分

上述即为实验内容，相关代码及文件同时上传至github
